
# ğŸ–¼ï¸ Image Caption Generator

Automatically generate human-like captions for images using deep learning. This project combines computer vision and natural language processing to describe images in natural English. Bonus: it can speak the caption out loud!

---

## ğŸš€ Features

- ğŸ“· Upload an image and get a caption generated automatically.
- ğŸ”ˆ Convert the caption into audio using text-to-speech (gTTS).
- ğŸ¤– Uses a custom-trained Encoder-Decoder architecture with attention mechanism.
- ğŸ§  Trained on the **Flickr8k** dataset with **Karpathy splits**.
- ğŸ—£ï¸ Tokenized with spaCy and saved in a custom `Vocabulary` class.

---

## ğŸ§± Project Structure

```
image-captioning/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ static/
â”‚        â””â”€â”€ uploads/        # Stores uploaded images & audio files
â”‚   â”œâ”€â”€ inference.py         # Inference logic and model loading
â”‚   â”œâ”€â”€ routes.py            # Web routes for Flask app
â”‚   â”œâ”€â”€ utils.py             # Helper functions like save_uploaded_image(), save_caption_audio()
â”‚
â”œâ”€â”€ captioning/
â”‚   â”œâ”€â”€ generator.py         # Generates caption using beam search
â”‚   â”œâ”€â”€ visualize.py         # Visualize attention mechanism
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ checkpoint_best.pth  # Best model checkpoint
â”‚   â”œâ”€â”€ vocab.pkl            # Pickled vocabulary
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ app_config.py       # App-specific parameters
â”‚   â”œâ”€â”€ base_config.py      # Base config shared across models
â”‚   â”œâ”€â”€ data_config.py      # Data-specific parameters
â”‚   â”œâ”€â”€ train_config.py     # Train-specific parameters
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/               # Input dataset files
â”‚   â”œâ”€â”€ collate.py           # Collate class
â”‚   â”œâ”€â”€ data_analysis.py     # Functions for data analysis
â”‚   â”œâ”€â”€ dataloader.py        # DataLoader setup
â”‚   â”œâ”€â”€ vocab.py             # Vocabulary class
â”‚   â”œâ”€â”€ dataset.py           # Custom Image Captioning Dataset 
â”‚
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ setup.py              # Functions to get optimizers, schedulers, and criterion
â”‚   â”œâ”€â”€ train.py              # Training loop
â”‚   â”œâ”€â”€ validate.py           # Validation logic
â”‚   â”œâ”€â”€ evaluate.py           # Evaluation/metrics
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ node_modules/         # Dependencies installed by npm (auto-generated)
â”‚   â”œâ”€â”€ src/                  # Source code for the React frontend
â”‚   â”‚ â”œâ”€â”€ App.css             # Styles specific to the App component 
â”‚   â”‚ â”œâ”€â”€ App.jsx             # Main App component
â”‚   â”‚ â”œâ”€â”€ main.jsx            # Entry point that renders <App /> into the DOM
â”‚   â”‚   
â”‚   â”œâ”€â”€ eslint.config.js      # ESLint configuration for code linting and formatting
â”‚   â”œâ”€â”€ index.html            # HTML template used by Vite to inject the app
â”‚   â”œâ”€â”€ package.json          # Project metadata and dependencies
â”‚   â”œâ”€â”€ package-lock.json     # Exact dependency versions locked for consistent installs
â”‚   â”œâ”€â”€ vite.config.js        # Vite configuration for the build tool and dev server
â”‚   
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ image-caption-generation.ipynb
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ attention.py         # Attention class
â”‚   â”œâ”€â”€ encoder.py           # Encoder CNN (e.g., ResNet)
â”‚   â”œâ”€â”€ decoder.py           # Decoder LSTM with attention
â”‚  
â”œâ”€â”€ run.py                   # Main runner (Flask app)      
â”œâ”€â”€ train_run.py             # Main training script for captioning model (load, train, validate, evaluate) 
â”œâ”€â”€ requirements.txt         # Python dependencies required to run the backend
â””â”€â”€ README.md                # Youâ€™re reading it ğŸ˜„
```

---

## ğŸ§ª Requirements

Install dependencies:

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

---

## ğŸ“‚ Dataset Setup

1. Download the **Flickr8k** image dataset and **Karpathy splits**:
   - Place images in: `data/flickr8k/images/`
   - Place JSON in: `data/flickr8k/dataset_flickr8k.json`

2. First run will tokenize captions and create the vocabulary.

---

## ğŸ Running the App

### ğŸ’» Train the model
```bash
python run_train.py
```

### ğŸ–¼ï¸ Run inference (Flask)
```bash
python run.py
```

### ğŸš€ Run UI (REACT)
```bash
cd ./frontend
npm run dev
```

Then upload an image to get its caption and audio version.

---

## ğŸ”§ Configuration

Edit configs in the `configs/` directory to tweak model settings, learning rates, beam size, etc.

---

## ğŸ“ˆ Model Architecture

- **Encoder**: Pretrained ResNet-101 (fine-tunable)
- **Decoder**: LSTM with attention
- **Tokenizer**: spaCy-based with custom `Vocabulary` class

---

## ğŸ“£ Credits

- Dataset: [Flickr8k](https://github.com/jbrownlee/Datasets)
- Tokenizer: [spaCy](https://spacy.io/)
- TTS: [gTTS](https://pypi.org/project/gTTS/)
- Inspired by the [Show, Attend and Tell](https://arxiv.org/abs/1502.03044) paper

---

## ğŸ§  Future Improvements

- Replace GRU with Transformer decoder
- Support for multilingual captions

---

## ğŸ›¡ï¸ License

MIT License. Do cool stuff, just give credit. ğŸ™Œ
