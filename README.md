
# ğŸ–¼ï¸ Image Caption Generator

Automatically generate human-like captions for images using deep learning. This project combines computer vision and natural language processing to describe images in natural English. Bonus: it can speak the caption out loud!

---

## ğŸš€ Features

- ğŸ“· Upload an image and get a caption generated automatically.
- ğŸ”ˆ Convert the caption into audio using text-to-speech (gTTS).
- ğŸ¤– Uses a custom-trained Encoder-Decoder architecture with attention mechanism.
- ğŸ§  Trained on the **Flickr8k** dataset with **Karpathy splits**.
- ğŸ—£ï¸ Tokenized with spaCy and saved in a custom `Vocabulary` class.

---

## ğŸ§± Project Structure

```
image-captioning/
â”œâ”€â”€ static/
â”‚   â””â”€â”€ uploads/             # Stores uploaded images & audio files
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ static/
â”‚        â””â”€â”€ uploads/        # Stores uploaded images & audio files
â”‚   â”œâ”€â”€ model.py             # Inference logic and model loading
â”‚   â”œâ”€â”€ routes.py            # Web routes for Flask app
â”‚   â”œâ”€â”€ utils.py             # Helper functions like save_uploaded_image(), save_caption_audio()
â”‚
â”œâ”€â”€ captioning/
â”‚   â”œâ”€â”€ generator.py         # Generates caption using beam search
â”‚   â”œâ”€â”€ visualize.py         # Visualize attention mechanism
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ checkpoint_best.pth  # Best model checkpoint
â”‚   â”œâ”€â”€ vocab.pkl            # Pickled vocabulary
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ app_config.py       # App-specific parameters
â”‚   â”œâ”€â”€ base_config.py      # Base config shared across modes
â”‚   â”œâ”€â”€ data_config.py      # Data-specific parameters
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ collate.py           # Collate class
â”‚   â”œâ”€â”€ dataloader.py        # DataLoader setup
â”‚   â”œâ”€â”€ vocab.py             # Vocabulary class
â”‚   â”œâ”€â”€ dataset.py           # Custom Image Captioning Dataset 
â”‚
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ train.py              # Training loop
â”‚   â”œâ”€â”€ validate.py           # Validation logic
â”‚   â”œâ”€â”€ evaluate.py           # Evaluation/metrics
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ 
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ image-caption-generation.ipynb
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ attention.py         # Attention class
â”‚   â”œâ”€â”€ encoder.py           # Encoder CNN (e.g., ResNet)
â”‚   â”œâ”€â”€ decoder.py           # Decoder LSTM with attention
â”‚
â”œâ”€â”€ run.py                   # Main runner (Flask app or CLI)       
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                # Youâ€™re reading it ğŸ˜„
```

---

## ğŸ§ª Requirements

Install dependencies:

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

---

## ğŸ“‚ Dataset Setup

1. Download the **Flickr8k** image dataset and **Karpathy splits**:
   - Place images in: `data/flickr8k/images/`
   - Place JSON in: `data/flickr8k/dataset_flickr8k.json`

2. First run will tokenize captions and create the vocabulary.

---

## ğŸ Running the App

### ğŸ’» Train the model
```bash
python train.py
```

### ğŸ–¼ï¸ Run inference (Flask or CLI)
```bash
python run.py
```

Then upload an image to get its caption and audio version.

---

## ğŸ”§ Configuration

Edit configs in the `configs/` directory to tweak model settings, learning rates, beam size, etc.

---

## ğŸ“ˆ Model Architecture

- **Encoder**: Pretrained ResNet-101 (fine-tunable)
- **Decoder**: LSTM with attention
- **Tokenizer**: spaCy-based with custom `Vocabulary` class

---

## ğŸ“£ Credits

- Dataset: [Flickr8k](https://github.com/jbrownlee/Datasets)
- Tokenizer: [spaCy](https://spacy.io/)
- TTS: [gTTS](https://pypi.org/project/gTTS/)
- Inspired by the [Show, Attend and Tell](https://arxiv.org/abs/1502.03044) paper

---

## ğŸ§  Future Improvements

- Replace GRU with Transformer decoder
- Support for multilingual captions
- Deploy as a full web app with audio player
- Add caption beam search UI

---

## ğŸ›¡ï¸ License

MIT License. Do cool stuff, just give credit. ğŸ™Œ
